%proo\documentclass[acmtog,nonacm]{acmart}
\documentclass{tstextbook}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,positioning,chains, shapes.geometric}
\usepackage{tipa}
\usepackage{color}
\usepackage{listings}
\makeatletter
\def\lmtt@use@light@as@normal{}
\usepackage{etoolbox}
%\usepackage{amsmath}
%\theoremstyle{plain}% default
%\newtheorem{thm}{Theorem}[section]
%\theoremstyle{definition}
%\newtheorem{defn}{Definition}[section]

\usepackage{booktabs}

\input{vc.tex}


\usepackage[lastexercise,answerdelayed]{exercise}
\setlength{\Exesep}{.5ex}
\setlength{\Exetopsep}{1em}
\renewcommand{\ExerciseListName}{Exercise}
\renewcommand{\AnswerListName}{}
\renewcommand{\ExerciseHeaderTitle}{(\emph{\ExerciseTitle.})\ }
\renewcommand{\ExerciseListHeader}{\ExerciseHeaderDifficulty%
\textbf{\ExerciseListName\ExerciseHeaderNB.}\ \ExerciseHeaderTitle%
\ExerciseHeaderOrigin\ignorespaces}
\renewcommand{\AnswerListHeader}{\textbf{\ExerciseHeaderNB.\ }}

\title{Additional Notes for Algorithms and Data Structures about Notation for Analysis of Algorithms}
\author{Thore Husfeldt}
\date{\small Revision {\tt \GITAbrHash}$\ldots$, \GITAuthorDate, \GITAuthorName}
\begin{document}
\maketitle

\chapter{Big Oh}


\begin{summary}
This note is about notation for the growth of functions encountered in the analysis of algortithms, sometimes called asymptotic\footnote{%
  The term “asymptotic” is highly misleading, it means “not falling together”, highlighting the fact that the function \(x\mapsto 1/x\) never touches the x-axis (its “asymptote”).
  This aspect of \emph{not touching} plays no role for us at all, it is in fact \emph{false}: 
  Our asymptotic notions are reflexive, so a function is asymptotically itself in that sense.
  You are advised to ignore your knowledge of Greek and stick to calling it ``$O$-notation''.}
notation.

Following Rutanen, we introduce Big-Oh notation as linear dominance, instead of asymptotic linear dominance.
The current version is of a highly preliminary nature.
\end{summary}


\section{Asymptotic bound: Big Oh}

The notation \(f\in O(g) \), is a “fuzzy version of \(f\leq g\).”
It is defined in terms of the \emph{set of functions} $O(g)$, which one can think of as all function that are ``at most $g$ times a positive constant'', i.e., 
\[ O(g) = \{ \ldots, \sqrt{g}, \ldots, \tfrac{1}{100}g,  \ldots,g, \ldots,\pi g, \ldots,235g\ldots\}\,.\]

We start with a precise definition for functions that are defined on the set of positive integers $\mathbf N =\{1,2,\ldots, \}$.
For many applications, this is completely sufficient.

\begin{definition}[$O$ as linear  dominance, monovariate on positive integers]
  \label{def:Omonovariate}
  Let $f, g\colon \mathbf N\rightarrow\mathbf R_{\geq 0}$.
  Then $f \in O(g)$ if there exist $c>0$ such that
  \[ f(n)\leq cg(n)\quad\text{for all $n\in \mathbf N$}\,. \]
\end{definition}

The notation \(f \in O(g)\) is often read as “eff is big-oh of gee“, or just “eff is oh of gee”, or “eff is order of  gee.'' 

\begin{example}
Some usage examples are:
  \[ n + 1 \in O(n) \quad\text{for $n\in\mathbf N$}\]
  and
  \[ n^2 -n + 5 \in O(n^2) \quad\text{for $n\in\mathbf N$}\]
and also
\[ 2n \in O ( n) \quad\text{for $n\in\mathbf N$}\,,\]
but  
\[ n^2 - n + 5 \notin O(n) \quad\text{for $n\in\mathbf N$}\,. \] 
\end{example}

As a convention, we will often  include the ``$n\in\mathbf N$'' for clarity (even though it is implicit in Definition~\ref{def:Omonovariate}) in this section.


\begin{theorem}[As a relation]
  \label{thm: Oh as a relation}
The relation between \(f\) and \(g\) when \(f=O(g)\) is
  \begin{description}
    \item[reflexive:] \( f \in O(f)\)
    \item[transitive:] if \( f\in O( g)\) and \(g\in O(h)\) then \(f\in O( h)\).
  \end{description}
\end{theorem}

The $O$-relation is \emph{not} symmetric.
For instance, $n^2\notin O(n)$ (even though $n\in O(n^2)$.)

\begin{theorem}[Properties] 
  \label{thm: Oh rules}
  \begin{description}
    \item[order-consistent] If \(f\leq g\) then \(f \in O(g)\).
    \item[scale-invariance] \(O(\alpha f) = O(f)\) for every real number $\alpha > 0$.
    \item[additive] \( O(f)+ O( g) = O(f + g)\).
    \item[multiplicative] \( O(f)\cdot O( g) = O(f \cdot g)\).
    \item[homogenuous] $fO(g) = O(fg)$.
    \item[monotone] If $f\leq g$ then $O(f) \subseteq O(g)$.
    \item[sum is max] $O(f + g) = O(\max \{f, g\})$
    \item[max is sum] $\max\{O(f) , O(g)\} = O(f) + O(g)$
  \end{description}
\end{theorem}

\begin{proof}
  If $f\leq g$ then in particular $f(x)\leq 1\cdot g(x)$, so $f\in O(g)$.

  The other proofs are tedious but easy.
  For instance, let us show  $O(f)+ O( g) \subseteq O(f + g)$.
  Consider $h\in O(f) + O(g)$.
  This means there is $f'\in O(f)$ and $g'\in O(g)$ such that $h=f'+g'$.
  Since $f'\in O(f)$ there exists $c_1>0$ such that $f'(n)\leq c_1 f(n)$ for all $n\in \mathbf N$.
  Similarly,  there exists $c_2>0$ such that $g'(n)\leq c_2g(n)$ for all $n\in \mathbf N$.
  Thus, we have $h(n) = f'(n) + g'(n) \leq c_1f(n) + c_2 g(n) \leq c\cdot (f(n) + g(n)) = c\cdot (f+g)(x)$ for $c=\max\{c_1,c_2\}$.
  In particular, $h\in O(f + g)$
\end{proof}

\begin{example}
  For $n\in\mathbf N$,
  \begin{enumerate}
    \item $\frac12 n \in O(n)$
    \item $n + 1\in O(n)$
    \item $3n \cdot \log ( n^6) \in O(n\log n)$
    \item $170 n \in O(n)$
    \item $170 n + 10 n \in O(n)$
    \item $170 n + \log n \in O(n)$
    \item $\ln n \in O(\log n)$
    \item $\log n \in O(\ln n)$
    \item $\log n \in O(n)$
    \item $\sqrt n \in O(n)$
    \item $n \in O(1 + n\log n)$
    \item $n \in O(n^2)$
    \item $(\log_2 n)^3 \in O(n^{1/4})$
    \item $\frac16 n^3 \in O(n^3)$
    \item $\binom{n}{3} \in O(n^3)$
    \item $2^n \in O(3^n)$.
    \item $n^{16} \in O(2^n)$.
  \end{enumerate}

  Most of these are mindless calculus exercises.
  For instance, $\log n \leq n$ for $n\in\mathbf N$ can be proved by induction or calculus (or eye-balling a function plot), at which point order-consistency applies.
\end{example}

\begin{example}
  \begin{enumerate}
\item $n^2\notin O(n)$
\item $n^2\notin O(n\log n)$
\item $n\notin O(\log n)$
\item $n^{1/4}\notin \log n$
\item $3^n\notin O(2^n)$
  \end{enumerate}
\end{example}

The behaviour of $O$ on polynomials can be summarised in the following useful result:

\begin{theorem}
   $a_kn^k +a_{k-1}n^{k-1} + \cdots + a_1 n + a_0 \in O(n^k)$ for $n\geq 1$.
\end{theorem}

\begin{proof}
  Let $a=\max\{a_0,\ldots, a_k\}$. 
  Then every term is bounded as $a_in^i \leq an^i$.
  Moreover, $n^i = n^i 1^{k-i}\leq n^i\cdot n^{k-i} = n^k$ for all $i$ because $n\geq 1$.
  Thus, \[
    a_kn^k +a_{k-1}n^{k-1} + \cdots + a_1 n + a_0  \leq
    an^k +an^k + \cdots + ka n^k + k an^k = kan^k \in O(n^k).\]
\end{proof}


\begin{example}
 We have
  \[ \binom{n}{2} \in O(n^2)\quad n\in\mathbf N\,,\]
  because  $\binom{n}{2} = \frac12n (n-1)$.
\end{example}


\begin{example}[Warning $\star$]
  \label{ref:binowarning}
  Note that according to Definition~\ref{def:Omonovariate}, we have $n^2 \notin O(\binom{n}{2})$ for $n\in\mathbf N$.
  (This is because $\binom{1}{2}= 0$, so there is no way to make $1^2 \leq c \binom{1}{2}$ no matter how large $c$ is chosen.)
\end{example}

\begin{exercise}
  Give a $O$-estimate for $\binom{n}{3}$.
\end{exercise}

\subsection{Non-examples}

Here are some examples of $f\notin O(g)$.

\begin{example}
  $4^n \notin O(2^n)$.

  This is a popular misunderstanding, possibly induced by the (correct) claim that $2\cdot 2^n\in O(2^n)$ and some confusion of $2\cdot (2^n)$ and ${(2\cdot 2)}^n$.

  Another (faulty) argument is idea of applying the logarithm to both $4^n$ and $2^n$, arriving at $n\log 4$ and $n\log 2$ and using the (correct) statement that $n\log 4\in O(n\log 2)$.
  That reasoning is not valid.
  
  Yet another mistaken argument is to reason as follows:
  Define function $f(n)=2^n$.
  Then $f(2n) = 2^{2n} = 4^n$. (Correct so far.)
  But for every constant $\alpha$, we have $f(\alpha n) =^?  O(f(n))$, so $4^n = f(2n) =^? O(f(n))=O(2^n)$.
  But no such rules exist, and I've marked the false statements with in the above argument with `?'.

\medskip
  To see that indeed $4^n\notin O(2^n)$, assume the opposite, namely $4^n\in O(2^n)$.
  Then, by definition there exists a constant $c>0$ such that $4^n\leq c2^n$ for all $n\geq 1$.
  Dividing by $2^n$ we have $c \geq 2^n$, but this is false for $n \geq \lceil\log_2 (c+1)\rceil$.
\end{example}

\begin{example}
  $n^2 \notin O(n)$.

  This is just plain false, despite its symmetrical claim, $n\in O(n^2)$ being true.
\end{example}


The above counterexamples hold no matter which definition of $O$ you use.
The following counterexample is specific for \emph{our} definition.

\begin{example} $\star$
  Define $O$ as in Definition~\ref{def:Omonovariate}.
  Then $n\notin O(n\log n)$ for $n\geq 1$.
  
  (Why? If $n\leq c n\log n$ for all $n \neq 1$ then in particular, $1\leq c 1\log 1 = 0$, which is absurd.)

	
  Similarly, $O(\log n) \neq O(1 + \log n)$ for $n\geq 1$.
\end{example}

\section{Asymptotic Equality: Big Theta}

The notation \(f\in \Theta(g) \) is a “fuzzy version of \(f=  g\)” or a “symmetric version of $O$.”

\begin{definition}
  $f\in \Theta(g)$ if $f\in O(g)$ and $g\in O(f)$.
\end{definition}

\begin{example}
  For $n\in \mathbf N$, some usage examples are:
\[ n + 1 \in \Theta(n), \quad n^2 - n + 5 \in \Theta( n^2), \] 
and also
\[ 2n \in \Theta ( n)\,,\]
but  
\[ n^2 - n + 5 \notin  \Theta(n)\,. \] 
\end{example}

\begin{theorem}[as a relation]
  \label{thm: Oh as a relation}
The relation between \(f\) and \(g\) when \(f\in \Theta(g)\) is an equivalence relation:
  \begin{description}
    \item[reflexive:] \( f \in \Theta(f)\)
    \item[symmetric:] if \( f \in \Theta(g)\) then \(g \in \Theta(f)\)
    \item[transitive:] if \( f\in \Theta ( g)\)  and   \(g= \Theta(h)\) then \(f= \Theta( h)\).
  \end{description}
\end{theorem}



\section{Conventions}


The curious expression \(O(1)\) means “any positive constant.” 
Directly from the defintion it is the set of functions $f$ such that there is a number $c>0$ with $f(n)\leq c\cdot 1$.
Examples are $\cos$ and $\sin$ (both of which are bounded by $1$.
Typically, it is used for ``some constant, maybe $17$ or $18$ or something.''

Expressions like $O(3n)$ makes sense, but are confusing and unprofessional and should be avoided; the expression $O(3n)$ means the same as $O(n)$.

Similarly, don’t write ``the number of comparisons is $\sim 3n^2  - 5n +7$.''
While it may not be \emph{false}, this expression signals lack of mastery of the notation, you should write
``the number of comparisons is $\sim 3n^2$.''

Similarly, you should reduce expressions inside a $O$:
The expression $O(n^3 + n^2(n + 16n + \log n))$ is equal to $O(n^3)$;
the two expressions mean exaclty the same.
Since the whole point of $O$ was to reduce clutter, the latter form is strongly preferred.
(The former form signals limited understand of how and why the notation is used.)

Most users of this notation are mainly interested in performance guarantees in terms of upper bounds, so one often sees $O$ used where $\Theta$ would have been just as appropiate.



\section{Daily Use}

\subsection{Basic programme analysis}


\begin{example}
Here is a programme that counts the $0$s in an array $a$ of length $n\geq 1$:

\begin{quotation}
  \begin{tabbing}
    xxxx\=xxxx\=xxxx\=\kill
    $c \leftarrow{} 0$\\
    \textbf{for\ } $i\in \{1,\ldots,n\}$\\
    \> \textbf{if\ } $a[i] = 0$\\
    \>\> $c\leftarrow{} c + 1$\\
  \end{tabbing}
\end{quotation}

We want to determine the number of assignments.

The first line use exacly $1$ assignment.
The iteration is more interesting.
Depending on the value of $a[i]$, the \textbf{if}-branch costs $0$ or $1$ assignments.
In the worst case, this is $\max\{0,1\} = 1$, which is the worst-case cost for the body of the loop.
The \text{for}-expression implictly stands for $n$ assigments $i\leftarrow 1$, $\ldots$, $i \leftarrow n$ and executes the body $n$ times, for a total of $2n$ assignments.

(Arguably, these analyses are so banal that it's hard to see what the notation accomplishes.
It gets more interesting now.)

Adding the $1$ contributions from the first line we arrive at $2n+1=  O(n)$ for $n\geq 1$.
We used $O$-notation only at the end, to simplify the expression.
We could also have written  $2n+1= \Theta(n)$
\end{example}


\begin{example}
Let us make the above piece of code slighly more interesting to show off $O$.
Assume now that $a$ is some data structure containing $n$ elements, with $n\geq 1$.
Further assume that $a.\mathit{popleft}()$ is known to take $O(m)$ time when $a$ contains $m$ elements.
(Think of $a$ as an array-based list.)
The $O(m)$ bound may come from a previous analysis, or a textbook, or the library documentation.

\begin{quotation}
  \begin{tabbing}
    xxxx\=xxxx\=xxxx\=\kill
    $c \leftarrow{} 0$\\
    \textbf{for\ } $i\in \{1,\ldots,n\}$\\
    \> \textbf{if\ } $a.\mathit{popleft}() = 0$\\
    \>\> $c\leftarrow{} c + 1$\\
  \end{tabbing}
\end{quotation}

Evaluating the \textbf{if}-statement now costs $O(m)$ for $m\in \{n, n-1,\ldots, 1\}$.
With foresight, we bound each of them as $O(n)$, using the fact that $ m \leq n$ so that $O( m)\subseteq  O( n)$ by monotonicity of $O$.
Including the $1$-contribution from the assigment, the whole \textbf{if}-statement costs $1+O(n) = O(1) + O(n)= O(n + 1) = O(\max{n, 1})=  O(n)$.
The \textbf{for}-loop now costs $n \cdot (1 + O(n))$, which simplifies to $O(n^2)$ for $n\in \mathbf N$.
Adding the contribution from the first line does not change this.
\end{example}

Note that in this example, it was crucial that we were freely able to manipulate the $O(m)$-expression for the beviour of \emph{popleft} that we inherited.

\subsection{Syntactic analysis}

\newcommand{\cost}{\operatorname{cost}}

It is possible to formalise the syntactic approach by writing down general rules for determining the cost  $\cost_n(P)$ of programme $P$ and parameter $n$.

\begin{description}
  \item[Assignment:] If $\cost_n(x\leftarrow a)$. All other atomic statements cost $0$.
  \item[Sequence:] If $P = Q; R$ with $\cost_n(Q) =f$, $\cost_n(R)= g$ then $\cost_n(P)= f + g$.
  \item[Selection:] If $P = \mathbf{if\ } b \mathbf{\ then\ } Q \mathbf{\ else\ } R$ with $\cost_n(Q) =f$, $\cost_n(R)= g$ and $\cost_n(b)= h$ then $\cost_n(P) = b + \max (f, g)$.
  \item[Iteration:] If $P = \mathbf{for\ }i\in I \mathbf{\ do\ }  Q$ then $\cost_n(P) = |I|(1 + \cost_n(Q))$.
\end{description}

However, there is no strong tradition for this degree of formalism in the analysis of algorithms, so we will not pursue it further.

\subsection{Conventions}

\begin{example}[Placeholder]
  When we say ``Insertion sort on $n\geq 1$ elements costs $O(n^2)$'', or ``The running time of Insertion sort on $n\geq 1$ elements is $O(n^2)$''  we mean ``The cost of Insertion sort on $n\geq 1$ elements is given by a cost function $f\in O(n^2)$.''
  This is just convenient type, despite the ``type error'' of pretending that a cost is a \emph{set of functions} rather than a function.
  A way of expressing this both pleasantly and properly would be ``The running time of Insertion sort on $n\geq 1$ elements \emph{belongs to} $O(n^2)$.''
\end{example}


\section{More about Logarithms}

We want to say something more general about the relationship between logarithms and power functions.
Such expressions appear frequently in the analysis of algorithms, for instance when comparing shellsort \(O(n^{3/2})\) to mergesort \(O(n\log n)\), or \(\sqrt{n}\)-trees to balanced binary search trees.


\begin{theorem}[ln versus power]
  For all exponents \(a>0\), \[ \ln n = O(n^a), \qquad(n\geq 1)\,.\]
\end{theorem}

Unfortunately, I know no short and easy proof for these relationships.

\begin{proof}
We have
\[
\frac{1}{y} \leq y \qquad(\text{for any } y \geq 1)\,.
\]
Thus, for any \(z\geq 1\),
\[
\int_1^z \frac{1}{y} \,dy\leq  \int_1^z y\, dy \,,
\]
so that \[ \ln z \leq \tfrac{1}{2} z^2\,.\]
In particular, if we choose \(z = n^{a/2}\),
then we have \( \ln n^{a/2} \leq \frac{1}{2} (n^{a/2})^2\), which simplifies to
\[  a\ln n \leq    n^a\qquad(n\geq 1)\,. \]
  In other words, $\ln n \leq \frac1a n^a$.
  Conclusion by order-consistency and scale-invariance of $O$.
\end{proof}

An even more general bound is the following:

\begin{theorem}[polylog versus power]
  For all  bases \(r> 1\) and exponents \(a>0\) and \(k> 0\), \[ (\log_r x)^k = O(x^a)\,.\]
\end{theorem}

\begin{proof}
  The proof is almost identical to the previous; set $z=n^{a/2k}$, so that 
  $\ln n^{a/2k} = \frac12(n^{a/2k})^2 = \frac12 n^{a/k}$.
  We arrive at \[ \frac{a}{k} \ln n \leq n^{a/k}\,,\]
  so $\ln n \leq (k/a)n^{a/k}$ and thus 
  \[ (\log_r n)^k = \bigl(\frac{\ln n}{\ln r}\bigr)^k \leq  \bigl(\frac{k}{a\ln r}\bigr)^k n^a\,;\]
  note that $r$, $a$, and $k$ are constants so scale-invariance makes short work of them.
\end{proof}


\section{Big Oh Defined in General}

We now define $O$ for the general case, in terms of linear dominance.

\begin{definition}[$O$ as linear dominance]%
  \label{def:Olineardominance}
  Let $X$ be a set and consider $f, g\colon X\rightarrow \mathbf R_{\geq 0}$.
Then $f= O(g)$ if there exists a positive real number $c>0$  such that
that \( f(x)\leq cg(x)\) for all $x\in X$.
\end{definition}

Definition~\ref{def:Omonovariate} is the special case $X=\mathbf N$ and suffices for many analyses.
We need the full definition in two contexts that often arise.

\begin{description}
    \item [Restrict the domain.] For some functions, we need to be able to restric the domain, for instance, we want to talk about $O(\log n)$ or $O(\binom{n}{2})$, both of which are problematic for $n=1$.
      (They are both $0$, so under the simplified Definition~\ref{def:Omonovariate}, we unfortunately have both $1\notin O(\log n)$ and $n^2\notin O(\binom{n}{2})$, because of the behaviour at $n=1$, neither of which is helpful.)
      We can solve this by setting $X=\{2,3,\dots\} = \mathbf N-\{1\}$.
    \item [Extend the domain.] Many data structures and algorithms need expression in two variables, such as the number $n$ of vertices and the number $m$ of edges, or the number $k$ of queries to a data structure of size $n$.
      In those situations, we want to set $X=\mathbf N\times \mathbf N$ and work with expressions such as $O(m(m+n))$ or $O(k(1 + \log n))$.
\end{description}


\subsection{Daily use}

Specifying the domain $X$ sometimes leads to clumsy expressions, and we will prefer to define $X$ implictly by always stating bounds on our parameters.

\begin{example}[Implicit domain specifation]
  We want two write human-readable expressions like the following:

  \begin{quote}
  {\bf Claim.} Mergesort of $n\geq 2$ elements takes time $O(n\log n)$.
  \end{quote}

  What this claim expresses more formally is that if $X=\{2,3,\ldots\}$ and $f\colon X\rightarrow\mathbf R_{\geq 0}$ is such that $f(n)$ denotes the cost of Merge Sort on instances of size $n\in X$ then $f\in O(n\log n)$.
\end{example}

Our use here aims to strike a compromise between on one side readibility and consistency with established notation, and on the other side precision and meaning.
A formally satisfying development of Definition~\ref{def:Olineardominance} would require separate $O$-sets for every domain $X$,encumbering the notation with expressions like $O_X$, $O_{\mathbf N\times N}$.
The reader is referred to the thesis of Rutanen for such an approach.


\begin{example}[Careful with small $n$]
  Note that the stronger claim that ``Mergesort on $n\geq 1$ elements takes time $O(n\log n)$'' in particular means that it takes \emph{no} time for $n=1$.
  This may be true or not, depending on the cost function or implementation. 
  (For instance, an implementation may indeed spend zero array accessses when it can determine that $n=1$.)
  But we have to be careful.
\end{example}

\subsection{Important functions}

\begin{example}[Binomial coefficient revisited]
  We return to the example from \ref{ref:binowarning} and finally establish
  \[ \binom{n}{2} = \Theta(n^2) \quad\text{for } n\geq 2\,.\]

  It remains to show $n^2\in O(\binom{n}{2})$ for $n\geq 2$.
  But if $n\geq 2$ then $n-1 \geq \frac12n$, so
  $\binom{n}{2} = \frac12n(n-1) \geq  \frac14n^2$.
  Thus, $n^2\leq 4\binom{n}{2}$, so $n^2\in O(\binom{n}{2})$ from the definition (with $c=4$).
\end{example}


\begin{example}[Expressions with logarithms]
  The following hold for $n\geq 2$:
  \begin{enumerate}
    \item $1\in O(\log n)$
    \item $1\in O(\log^* n)$
    \item $n\in O(n\log n)$
  \end{enumerate}
  The following hold for $n\geq 3$:
  \begin{enumerate}
    \item $1\in O(\log\log n)$
    \item $n\in O(n\log\log n)$
  \end{enumerate}
\end{example}

\subsection{Multivariate functions}

In the following example, assume that $a$ has length $m\geq n$ and recall that we assume \emph{popleft} takes $O(s)$ time if $a$ has $s$ eleements.

\begin{quotation}
  \begin{tabbing}
    xxxx\=xxxx\=xxxx\=\kill
    $c \leftarrow{} 0$\\
    \textbf{for\ } $i\in \{1,\ldots,n\}$\\
    \> \textbf{if\ } $a.\mathit{popleft}() = 0$\\
    \>\> $c\leftarrow{} c + 1$\\
  \end{tabbing}
\end{quotation}

Assume $n\geq 1$.
The costs for each of the  $n$  executions of the \emph{if}-statement are $O(m)$, $O(m-1)$, $\ldots$, $O(m-n)$, respectively, which we bound by $O(m)$.
The cost for increasing $c$ does not change this, since $m\geq n\geq 1$.
The total cost becomes $1 + n\cdot O(m) = O(nm)$.

\medskip
What happens \emph{formally} in the above analysis is that $X = \mathbf N\times \mathbf N$ and the cost function has the form $f\colon \mathbf N\times \mathbf N$.
Specifically, the cost of the \textbf{if}-statement is $f_{\textbf{if}} (n,m) \in O(m - n)\subseteq O(m)$.
The cost of the \textbf{for}-loop is $f_{\textbf{for}}(n, m) = n\cdot f_{\textbf{if}} (n,m) \in  n\cdot O(m)=O(mn)$.
This kind of precision is not normally used.


\section{A word of warning}

Definition~\ref{def:Olineardominance}  is \emph{not} the standard definition of $O$ found in most textbooks.
(We define $O$ in terms of ``linear dominance''; the standard definition uses ``asymptotic linear dominance on $\mathbf N$''.)

For completeness, here is the standard definition:

\begin{definition}[Asymptotic linear dominance]%
  \label{def:ald}
  Let $f, g\colon \mathbf N\rightarrow \mathbf R_{\geq 0}$.
  Then $f= O_{\mathrm a}(g)$ if there exists a positive real number $c>0$ and an integer $N\in \mathbf{N}$ such that
that \( f(n)\leq cg(n)\) for all $n \geq N$.
\end{definition}

(There are a number of immaterial variations of the  definition (using $\mathbf N_0=\{0,1,\ldots\}$ instead of $\mathbf N$, or requiring $|f(n)|\leq cg(n)$.))

For functions like $n$, $n^2$, it does not matter whether we use Definition~\ref{def:Omonovariate} or \ref{def:ald}, indeed $O(n^2)= O_{\mathrm a}(n^2)$ for $n\in \mathbf N$.
The difference appears mainly in the behaviour of functions that vanish for small $n$, such as $\log n$, $n\log n$, and $\binom{n}{2}$.
For instance $\log 1=0$, $1\log 1 =0$, and $\binom{1}{2}=0$ so that  $O(n\log n)\neq O_{\mathrm a}(n\log n)$ for $n\in \mathrm N$.

If we were using the standard definition~\ref{def:ald}, we could gloss over these details.
However, in daily use, a stipulation of ``for large enough $n$'' is understood and few practictioners would ever raise an eyebrow at seeing the expression $n\in O(n\log n)$ without the (formally crucial) restriction to $n\geq 2$ or much care about which definition has been used.

\chapter{Tilde}


\begin{summary}
  This section was an attempt to carefully explain limits before tildes, because limits were also useful to define $O$ (or so it seemed to be before 2022.)

  By the much better definition in the previous chapter, this all may be a waste of effort.
  In any case, it's a draft.
  Don't trust a word of it.
\end{summary}



\section{Asymptotic equality: tilde}

The notation \(f(x)\sim g(x)\), sometimes abbreviated to \(f\sim g\), is a “fuzzy version of \(f=g\).”

\begin{definition}[Tilde, informally]
We write \(f(x)\sim g(x)\) when \(f(x)/g(x)\) approaches \(1\) as \(x\) grows.
\end{definition}


\begin{example}
Som usage examples are
\[ n + 1 \sim n\,, \quad \binom{n}{3}\sim \textstyle\frac16n^3\,,\quad 3n\log n + 100 n \sim 3 n\log n\,. \] 
To get a feeling for this, evalute the corresponding fractions for $n=1,10,100,1000,\ldots$ and
  and use your good judgement.
For instance, $n+1\sim n$ because \[
  \frac{n+1}{n} \quad \text{grows as}\quad \frac{2}{1} =2, \frac{11}{10}=1.1, \frac{101}{100}=1.01, \frac{1001}{1000}=1.001,\ldots\,.
\]
Expecting no dramatic changes for larger $n$, we can confidently state that this sequence of numbers approaches $1$.

On the other hand
\[ 2n \not\sim n,\quad n^2 - n + 5 \not\sim n\,. \] 
For instance, \[
  \frac{2n}{n} \quad \text{grows as}\quad \frac{2}{1}, \frac{20}{10}, \frac{200}{100}, \frac{2000}{1000},\ldots
\]
all terms of which are equal to $2$.
This does not approach $1$. 
(Instead, it approaches $2$.)
\end{example}

\begin{exercise}
  Argue by by manual computation, using an electronic calculator, or writing a programme, that
  \begin{enumerate}
    \item
      $n\sim n-1$
    \item
  $\binom{n}{3}\sim \textstyle\frac16n^3$.
    \item
  $3n\log_2 n + 100 n \sim 3 n\log_2 n$.
    \item
      $\sqrt n\not\sim n$.
    \item
   $n^2 - n + 5 \not\sim n$.
    \item
      $\log_2 n\not\sim n$.
    \item
      $7\not\sim 5$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  True of false?
  \begin{enumerate}
    \item
      $n\sim n+ \frac{1}{n}$
    \item
  $\log_2 n\sim \ln n$
    \item
      $n^{10000} \sim  n^{10001}$
  \end{enumerate}
\end{exercise}


The notation \(f \sim g\) is read as “eff is asymptotically equal to gee”, or just “eff is tilde gee” among friends.

\begin{theorem}[as a relation]
  The relation \(\sim\) is an equivalence relation on the set of real functions.
  That is, it is
  \begin{description}
  \item[symmetric:] \( f\sim g \) if and only if \( g\sim f\)
  \item[reflexive:] \( f\sim f\)
  \item[transitive:] if \( f\sim g \) and \(g\sim h\) then \(f\sim h\)
  \end{description}
\end{theorem}

\begin{theorem}[Rules]
  \label{prop: tilde rules}
  Tilde notation satisfies the following rules:
  \begin{enumerate}
    \item If \(f_1\sim g_1\) and \(f_2 \sim g_2\) then \(f_1 \cdot f_2 \sim g_1 \cdot g_2\).
    \item \(n +C\sim n\) for every constant \(C\)
    \item \(n + \log_a n\sim n\) for any base $a>1$.
    \item \label{item: tilde polynomial}
      \(a_k n^k + a_{k-1} n^{k-1} + \cdots + a_1 n + a_0 \sim a_k n^k\)
    \item \(\lfloor x \rfloor \sim \lceil x \rceil \sim x\)
  \end{enumerate}
\end{theorem}


\begin{remark}
  On the other hand,
\begin{enumerate}
  \item $\log_b n \not\sim \log_a n$ for $a\neq b$
  \item $C  n \not\sim n$ for every constant $C\neq 1$
  \item $n^a \not\sim n^b$ for $a\neq b$
\end{enumerate}
\end{remark}

In summary, think of \(\sim\) as some fuzzy equality symbol.
The most useful fuzziness is that we can disregard lower order terms (but have to pay attention to the constant in front of the highest-order term.)



\emph{Warning: the rest of this section is a draft and may contain errors.}

Tilde notation is defined in terms of limits, so we consider limits first.

\section{Limits}

Let $f$ be a function from $\mathbf R$ to $\mathbf R$.
Then the \emph{limit at infinity} of $f$ (or just \emph{limit} in these notes), written
\[ \lim_{x\rightarrow \infty} f(x)\]
is one of four things: a real number, $\infty$, $-\infty$, or undefined.
All our limits are at infinity, so we will drop the ``$x\rightarrow \infty$.''
Inuitively, the limit is ``what $f(x)$ approaches arbitrarily closely as $x$ grows.'' 
For instance,
\[ \lim \frac1x= 0, \qquad \lim\left( 1-\frac1x\right) = 1\,.\]
Pathologically, $\lim 7 =7$.
Not all functions have limits, for instance $\lim \sin x$ is meaningless and undefined.
(The function oscillates between $-1$ and $1$, never approaching a value.)

For all function we will encounter, the limit is easily determined.

The \emph{formal} definition is as follows.
For real number $c$ we say that $\lim f(x) =c$ if for every $\delta > 0$, there is a real number $b$ such that
\[ |f(x) - c|\leq \delta \qquad\text{for $x>b$}\,. \]
We say that $\lim f(x)= \infty$ if for every $B$ where is a real number $b$ such that \[f(x) > B\qquad\text{for $x>b$}\,.\]

\begin{theorem}[Laws for limits at infinity]
  Assume that $\lim f(x)$ and $\lim g(x)$ exist.
  \begin{description}
    \item[Constant:] $\lim c = c$ for any real $c$.
    \item[Constant factor:] $\lim a f(x) = a\lim f(x)$ for any real $a\neq 0$.
    \item[Additive:] $\lim (f(x) + g(x)) = \lim f(x) + \lim g(x)$.
    \item[Multiplicative:] $\lim (f(x) \cdot g(x)) = \lim f(x) \cdot \lim g(x)$.
    \item[Domination] If $f(x) \leq g(x)$  for $x> b$ then $\lim f(x) \leq \lim g(x)$.
  \end{description}
\end{theorem}

These laws sometimes give nonsensical results such as $\infty/\infty$ or $5/0$, in which case you can deduce nothing from them.


\begin{proof}[Some proofs]
  For the second claim, let $c= \lim f(x)$ and let $\delta >0$ be given.

  Choose $b$ such that $|f(x) - c|\leq \delta/|a|$ for $x\geq b$.
  Then we have 
  \[ |af(x) - ac|= |a(f(x) - c)| = |a|\cdot |f(x)-c| \leq |a|\frac{\delta}{|a|} = \delta\,.\]

  \medskip 
  To establish additivity, let $\lim f_1(x) = c_1$ and $\lim f_2(x) = c_2$.
  We want to prove $\lim f_1(x) + f_2(x) = c_1 + c_2$.
  Let $\delta > 0$ be given and choose $b_1$ and $b_2$ such that 
  \[ |f_i(x) - c_i| \leq \textstyle\frac12\delta \qquad\text{for $x\geq b_i$}\,.\]
  But then, for $x \geq \max{b_1 + b_2}$, we have
  \begin{multline*}
    |(f_1(x)+f_2(x)) - (c_1 + c_2)| = |f(x)-c_1 + f_2(x) -c_2|\leq \\
  |f(x)-c_1| + |f_2(x)-c_2| \leq \textstyle\frac12\delta + \frac12\delta = \delta\,,
  \end{multline*}
  using the triangle inequality (which states that $|x + y|\leq |x| + |y|$.)
  The proof for multiplicativity is similar, but algebraically more involved.
  The exponentiation proof for integral $n$ is induction.
\end{proof}

The calculation of limits is sometimes very easy (such as $\lim x$ or $\lim 7$ or $\lim 1/x$).
We also have
\begin{multline*}
  \lim (a_kn^k+\cdots +a_1n+ a_0 )= \\
  \lim a_kn^k\cdot \bigl(
  1 + \frac{a_{k-1}}{a_k n}  +\cdots +\frac{a_1}{a_k n^{k-1}}+ \frac{a_0}{a_k n^k} \bigr)\,,
\end{multline*}
 and observing that each of the terms in the parenthesis involving $n$ vanishes as $n$ tends to infinity, the entire expression equals $\lim a_kn^k$.

Sometimes, these claim use nontrivial calculus, even for the relatively harmless functions that appear in the analysis of algorithms.
For instance, depending on you mathematical maturity, arrogance, or sloppiness, the claim \[ \lim \frac{\ln n}{n} = 0,,\]
is either ``clear,'' requires you to enter the expression into a graphing calculator, write a computer program to calculate the value for large $n$, or come up with the following:

\begin{theorem}\label{prop: log vs linear limit}
  \[\lim\frac{\ln n}{n} = 0,.\]
\end{theorem}
\begin{proof}
  Using calculus, we have
  \[
    \ln n =
    \int_1^n \frac1y\, dy \leq
    \int_1^n \frac{1}{\sqrt y}\, dy =
    \left [ 2\sqrt y \right]_1^n< 
    2\sqrt{n} \,.
  \]
  Thus, 
  \[ \frac{\ln n}{n} \leq \frac{2\sqrt{n}}{n} = \frac{2}{\sqrt n}\,,\]
  which tends to $0$ as $n$ grows.

  (There are many other ways to prove this.)
\end{proof}

\paragraph{Discussion} Aren't we done? Can't we define the running time of an algorithm as $\lim T(x)$?
For instance, because of the polynomial property, the complicated running time of three nested for-loops, $\binom{n}{3}=\frac16n^3 - \frac12n^2+ \frac13n$, would be given as its limit,
\[\textstyle\lim \frac16n^3 - \frac12n^2+ \frac13n = \lim\frac16n^3\,.\]
Alas, this is not good enough, because $\lim\frac16n^3= \infty$.
In fact, all nonconstant running times ($\log n$, $n$, $\frac16n^3$, $2^n$, $\ldots$) have the \emph{same} limit (namely, infinity).
There would be only two different running times: constant and infinite.

\section{Tilde}

\begin{definition}[Tilde]
For functions, \(f, g\colon \mathbf R \rightarrow \mathbf R\) we say that \(f\) is \emph{asymptotically equal} to \(g\), in symbols \[f(x)\sim g(x)\]
if 
\[ \lim_{x \rightarrow \infty} \frac{f(x)}{g(x)}  = 1\,.\]
\end{definition}[Tilde]


\begin{example}
The above makes no sense whenever \(g(x)\) is zero infinitely often as \(x\) tends to infinity.
An example of such a function is \(\sin (x)\); it vanishes whenever \(x\) is a multiple of \(\pi\).
Luckily for us, such functions do not appear in the analysis of algorithms (running times are not 0), and we could proceed by just ignoring this issue.
Formalists will want to read every occurrence of “function” as a “function that is eventually nonzero,” i.e., “function  \(f\) for which there exists a value \(x_0\) such that \(f(x) \neq 0\) for \(x\geq x_0\).”
Such formalists will want to convince themselves that all functions appearing the analysis of algorithms (polynomials, logarithms, exponentials and their compositions) satisfy this requirement, except for cases such as \(n- n\).
Developing this carefully is neither hard nor illuminating, so we will overlook it.
\end{example}

\begin{theorem}
  \(\sim\) is an equivalence relation on the set of real functions.
  That is,
  \begin{enumerate}
    \item \(f(x)\sim g(x)\) iff \(g(x)\sim f(x)\)  (symmetry), 
    \item\(f(x)\sim f(x)\) (reflexivity),
    \item if \(f(x)\sim g(x)\) and  \(g(x)\sim h(x)\) then 
  \(f(x)\sim h(x)\) (transitivity).
  \end{enumerate}
\end{theorem}

\begin{proof}
  Directly from the definition and rules for limit.
  For instance, 
  \begin{multline*}
    \lim_{x\rightarrow \infty} \frac{f(x)}{h(x)} = 
    \lim_{x\rightarrow \infty} \frac{f(x)}{h(x)}\frac{g(x)}{g(x)} =
    \lim_{x\rightarrow \infty} \frac{f(x)}{g(x)}\frac{g(x)}{h(x)} =  \\
    \biggl(\lim_{x\rightarrow \infty} \frac{f(x)}{g(x)}\biggr)\cdot \biggl(\lim_{x\rightarrow \infty} \frac{g(x)}{h(x)} \biggr) =  
    1\cdot 1 
    = 1\,.
  \end{multline*}
  establishes transitivity except if you worry about $g(x) = 0$.
\end{proof}

\begin{theorem}[multiplication, division]
  If \(f_1(x)\sim g_1(x)\) and \(f_2(x)\sim g_2(x)\)  then 
  \begin{enumerate}
    \item
  \((f_1\cdot f_2)(x)  \sim (g_1\cdot g_2)(x)	\).
    \item
  \((f_1/ f_2)(x)  \sim (g_1 / g_2)(x)	\).
  \end{enumerate}
\end{theorem}
\begin{proof}
  Directly from the rules for limits. 
\end{proof}

\begin{theorem}[logarithm]
  If $\lim g(x)=\infty$ and  \(f\sim g\) then \(\log f \sim \log g\).
\end{theorem}
\begin{proof}
  Follows from Propositions~\ref{prop: tilde implies Oh} and \ref{prop: log big Oh} below.
\end{proof}

This does not work in the other direction: \(f\sim g\) does \emph{not} imply \(2^ f \sim 2^ g\).
It is not true in general that $f\sim g$ implies $\log f\sim \log g$.


Despite these handy tools, we often have to go back directly to the definition.
To establish 
\[n + \ln n \sim n\,,\]
we need the calculation
\[ \lim \frac{n+ \ln n}{n} = \lim\frac{n}{n} + \lim\frac{\ln n}{n} = 1 + 0= 1\,,\]
using the laborious argument from Proposition~\ref{prop: log vs linear limit}.


\chapter{Retired stuff}

Don't read this, it will burn your eyes and rot your soul.

\section{Why do this?}

We begin by introducing notation, intuition, and rules for use.
Precise definitions and explanation are given later.

First, though: Why are we doing this at all?
We need a toolset to compare functions, because we want to compare the running times of algorithms, which are given as functions (typically of the input size). 
We both want to be able to say ``$f$ is roughly equal to $g$,''  and ``$f$ is bounded by $g$`` for a certain granularity of ``roughly'' that is both precise enough to be interesting and coarse enough to be useful.

Note that we \emph{do} already a definition for have ``precisely equal,'' which is typically written as $f=g$ and defined as
\[ f(x) = g(x)\qquad\text{for all $x$}\,.\]
That was easy to define, but it is also quite useless for us; if $f(n) = n$ and $g(n)= n + 1$ then $f\neq g$ (but we want to treat those two running times as ``the same'').
Similarly we \emph{do} already a definition for ``bounded by,'' typically written as $f\leq g$ and defined as
\[ f(x) \leq g(x)\qquad\text{for all $x$}\,.\]
This is also quite useless: if $f(n) = n^2$ and $g(n)= n^3$ then $f\not\leq g$ because $(-1)^2 \not\leq (-1)^3$.
(But we really want to be able to say that a quadratic-time algorithm is better than a cubic-time one.)


\subsection{Big Oh as a limit}

\begin{theorem}
  Let $f,g\colon\mathbf R\rightarrow\mathbf R$ with $g$ nonnegative and such that $\lim f$ exists.
  Then \[ f=O(g) \]
 if and only if
  \[ \lim \frac{|f(x)|}{g(x)} < \infty \,.\]
\end{theorem}

\begin{proof}
  Assume $f(x)=O(g(x))$ and thus pick $b$ such that $|f(x)|\leq |g(x)| = g(x)$ for all $x>b$.
  In particular \[\frac{|f(x)|}{g(x)} \leq 1\,,\]
  so \[\lim \frac{|f(x)|}{g(x)} \leq 1< \infty\,.\]
  
  For the other direction, first assume
  \( \lim |f(x)|/ g(x) = B \)
  for some real number $B$.
  This means that there exist an integer $b$ such that for $x>b$,
  \[  \left|\frac{|f(x)|}{g(x)} - B\, \right| \leq 1.\]
  In particular,
  \[ \frac{|f(x)|}{g(x)} - B \leq 1\,,\]
  which can be rearranged to 
  \[ |f(x)| \leq (1+ B)g(x) = (1+B)|g(x)|\,.\]
  For the final case, i.e.,
  \( \lim |f(x)|/ g(x) = -\infty\,, \)
  we can repeat the above argument with $B=0$.
\end{proof}

We can view this an alternative definition of $O$, closer in spirit to the defintion of $\sim$, in that it expresses the relationship between $f$ and $g$ in terms of the limit of $f/g$.\footnote{
  A veritable curnucopia of related concepts exists and is used 
  For instance, when $\lim f/g =0$ we write $f=o(g)$.
  When $\lim g/f < \infty$ we write $f=\Omega(g)$.
  When $\lim f/g = B$ for some real number we write $f=\Theta(g)$.
}

After all this work, we can finally establish the relationship between $\sim$ and $O$:

\begin{theorem}\label{prop: tilde implies Oh}
  Let $f,g\colon\mathbf R\rightarrow\mathbf R$ with $g$ nonnegative and such that $\lim f$ exists.
  If $f\sim g$ then  \( f=O(g)\,. \)
\end{theorem}

We repeat that the other direction is not even close to being true: $2x =O(x)$ but $2x\not\sim x$.

\medskip

\section{Hints and pitfalls}

When I read summaries like the above, my brain can't stop subvocalising ``How very useful! Whatever makes the notation look neat is also be true; and somebody else has proved it for us. Why bother.''

So here are some warnings.

\begin{description}
  \item[Misapplied notation.] 
    Avoid writing ``the number of comparisons is $\sim 3x^2  - 5x +7$.''
    While not \emph{false}, this signals lack of mastery of the notation, you should write
    ``the number of comparisons is $\sim 3x^2$.''
    (The whole point of the notation is to allow you to avoid the smaller terms.)
    Similarly, don't write $O(n^2 + n)$, or $O(7n^2)$; do write $O(n^2)$, even though these expressions all mean the same. 
    (As does $O(100034245n^2 + n\log n +\sqrt n)$.)
    You're not doing yourself any favour (nor your reader) by not cleaning that up.
  \item[Everything is a constant.]
    Check this out: ``$1+2+3+\cdot + n$ is just a sum of constants.
    Each can be bounded by $O(1)$, so we can rewrite the sum to $O(1)+ O(1) +\cdot +O(1)=O(n)$.''
    The argument is false, and the conclusion is false as well.
    One thing wrong with the argument is that the terms are not constant, they depend on $n$.
    Indeed, half of them are larger than $\frac n/2$, which is certainly not $O(1)$.
    The other mistake is using $O$-expressions to the left of an equality sign:
  \item[Reading big-Oh backwards.]
    It is true that $2n =O(n)$ and $17n+4 =O(n)$.
    But this ``$=$'' does not behave as the one you're used to.
    So you cannot infer $2n=17n+4$, that's still false.
    In general, aim at having only a single $O$-in your calculations, and always at the end.
    Never write $O(n)= 3n$, it means nothing.
  \item[Exponents don't like asymptotics.]
    The argument ``$6^x = O(2^x)$ because $6$ is only a constant times $2$'' sounds tempting but the claim is simply \emph{false}.
    (In particular $6^x \neq 3\cdot 2^x$.)
  \item[Broken logarithm rule.]
    This may be nit-picking, but:
    Define $f,g\colon \mathbf R\rightarrow \mathbf R$ as $f(n)=1+\frac1n$ and $g(n)=1+\exp(-n)$ and 
    note that \[ 
    \lim f(n) = \lim g(n)= \lim \frac{f(n)}{g(n)} = 1\,\]
    so that both $f(n)\sim g(n)$ and $f(n)=O(g(n))$.
    However, using calculus one can verify
    \[ 
    \lim \frac{\log (1+\frac1n)}{\log(1+\exp(-n))} =
    \lim \frac{\log (\frac1n)}{\log\exp(-n)} =
    \lim \frac{\exp(n)}{n}=\infty\,
    \]
    so $\log f(x)$ is not $O(\log g(n))$.
    Compare this to Propositions~\ref{prop: log tilde} and \ref{prop: log O}.
    (It was important that $g(x)\rightarrow \infty$.)
\end{description}

\section{Multivariate asymptotics}

To be written.

The motivation is that we freely use expressions like $O(n+m)$ in the analysis of, say, graph algorihtms, but the concept of a \emph{bivariate} asymptotic notation is never defined.



\end{document}
